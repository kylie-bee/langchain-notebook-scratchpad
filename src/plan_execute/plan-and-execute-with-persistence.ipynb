{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan and Execute with Persistence using Pydantic models as State\n",
    "\n",
    "In this example, I've combined the Plan and Execute cognitive architecture with the state as Pydantic model and persistence examples. This means all nodes receive an instance of the current state as their first argument and it is validated before each node executes. It also shows how new input is received into the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiosqlite in c:\\users\\boome\\repos\\langchain-notebook-scratchpad\\.venv\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\boome\\repos\\langchain-notebook-scratchpad\\.venv\\lib\\site-packages (from aiosqlite) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -U langchain langchain_openai tavily-python aiosqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the tools\n",
    "\n",
    "We will first define the tools we want to use.\n",
    "For this simple example, we will use a built-in search tool via Tavily.\n",
    "However, it is really easy to create your own tools - see documentation [here](https://python.langchain.com/docs/modules/agents/tools/custom_tools) on how to do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilyAnswer\n",
    "\n",
    "tools = [TavilyAnswer()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "\n",
    "Now we need to load the chat model we want to use.\n",
    "Importantly, this should satisfy two criteria:\n",
    "\n",
    "1. It should work with messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them.\n",
    "2. It should work with OpenAI function calling. This means it should either be an OpenAI model or a model that exposes a similar interface.\n",
    "\n",
    "Note: these model requirements are not requirements for using LangGraph - they are just requirements for this one example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# We will set streaming=True so that we can stream tokens\n",
    "# See the streaming section for more information on this.\n",
    "model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After we've done this, we should make sure the model knows that it has these tools available to call.\n",
    "We can do this by converting the LangChain tools into the format for OpenAI function calling, and then bind them to the model class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# It is important to convert and bind correctly!\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model_with_tools = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the agent state\n",
    "\n",
    "The main type of graph in `langgraph` is the `StateGraph`.\n",
    "This graph is parameterized by a state object that it passes around to each node.\n",
    "Each node then returns operations to update that state.\n",
    "These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute.\n",
    "Whether to set or add is denoted by annotating the state object you construct the graph with.\n",
    "\n",
    "For this example, the state will track input, a plan, past steps, and the final response. This State allows the agent to keep track of their plan and the steps they have taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Annotated, TypedDict, List, Tuple, Sequence\n",
    "import operator\n",
    "\n",
    "# The overall state\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    # Holds thread of messages, human and AI responses\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # Couldn't this plan be a Plan object?\n",
    "    plan: List[str]\n",
    "    # Holds all past steps, including those from previous runs.\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "\n",
    "# The individual plan\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in the future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(description=\"Steps to follow in the future, should be sorted in order\")\n",
    "\n",
    "# The final response\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"Response to user\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Prompt Templates\n",
    "\n",
    "Several prompt templates are used depending on the step of the graph the agent is in. Different system messages combined with human input (when needed) are used to generate the proper thoughts at each step.\n",
    "\n",
    "## Input prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "executor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # The system message could be parameterized as well.\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        # This is where thread history is maintained\n",
    "        (\"placeholder\"), (\"{chat_history}\"),\n",
    "        # This is where user input is inserted\n",
    "        (\"human\"),(\"{input}\"),\n",
    "        # This is where state is kept.\n",
    "        (\"placeholder\"), (\"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial planning prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective and associated conversation, come up with a simple \\\n",
    "step by step plan. This plan should involve individual tasks, that if executed \\\n",
    "correctly will yield the correct answer. Do not add any superfluous steps. The result \\\n",
    "of the final step should be the final answer. Make sure that each step has all the \\\n",
    "information needed - do not skip steps.\n",
    "\n",
    "Conversation so far:\n",
    "{chat_history}\n",
    "\n",
    "Current objective:\n",
    "{objective}\"\"\"\n",
    ")\n",
    "planner = create_structured_output_runnable(\n",
    "    Plan, ChatOpenAI(model=\"gpt-4-turbo\", temperature=0), planner_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replanning prompt\n",
    "\n",
    "After a task is executed, we want to consider if our plan is on track and we may want to allow a human to insert additional input (if they've enabled the interrupt flag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import create_openai_fn_runnable\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective and associated conversation, come up with a simple step \\\n",
    "by step plan. This plan should involve individual tasks, that if executed correctly \\\n",
    "will yield the correct answer. Do not add any superfluous steps. The result of the \\\n",
    "final step should be the final answer. Make sure that each step has all the information \\\n",
    "needed - do not skip steps.\n",
    "\n",
    "The conversation so far is this:\n",
    "{messages}\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the following steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the \\\n",
    "user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan \\\n",
    "that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "# OPEN QUESTION: I do not understand why we are using the fn runnable here. I don't understand what it does\n",
    "# or why we need it.\n",
    "#\n",
    "# My current guess is that by passing in the Plan and Response models, it is encorcing to the agent that\n",
    "# the input and output should be of those types.\n",
    "replanner = create_openai_fn_runnable(\n",
    "    [Plan, Response], ChatOpenAI(model=\"gpt-4-turbo\", temperature=0), replanner_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence\n",
    "\n",
    "To add in persistence, we create a checkpoint. In this case we have to use the Async version of SQLlite to support async calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the graph\n",
    "\n",
    "## Define the nodes\n",
    "\n",
    "We now need to define a few different nodes in our graph.\n",
    "In `langgraph`, a node can be either a function or a [runnable](https://python.langchain.com/docs/expression_language/).\n",
    "There are three main nodes we need for this:\n",
    "\n",
    "1. Planning step: this is where the agent develops a plan of how to accomplish the goal given to it.\n",
    "2. Execution step: this is where the agent executes the next step of the plan. This may involve calling a tool, and as such, uses a subgraph. **NOTE**: The subgraph could have been created using a prebuilt sequence (i.e., `create_openai_functions_agent`) but for this example, we will create the subgraph manually.\n",
    "3. Replanning step: this is where the agent decides if it needs to replan. This is a conditional node that will either go back to the execution step or end the conversation and return the response.\n",
    "\n",
    "We will also need to define some edges.\n",
    "Some of these edges may be conditional.\n",
    "The reason they are conditional is that based on the output of a node, one of several paths may be taken.\n",
    "The path that is taken is not known until that node is run (the LLM decides).\n",
    "\n",
    "1. Conditional Edge: after the replanning step, if the agent decides to replan, it should go back to the planning step. If it decides to end the conversation, it should go to the end node.\n",
    "2. Normal Edge: after the planning and execution steps, it should always go back to the execution and replanning steps, respectively.\n",
    "\n",
    "Let's define the nodes, as well as a function to decide how what conditional edge to take.\n",
    "\n",
    "**MODIFICATION**\n",
    "\n",
    "We define each node to receive the AgentState base model as its first argument.\n",
    "\n",
    "### Subgraph for Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mimics the code of the `create_openai_functions_agent`` function in the `langchain.chains.openai_functions` module\n",
    "from typing import Union\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain.agents.format_scratchpad.openai_functions import (\n",
    "    format_to_openai_function_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_functions import (\n",
    "    OpenAIFunctionsAgentOutputParser,\n",
    ")\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "agent_runnable = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        )\n",
    "    )\n",
    "    | executor_prompt\n",
    "    | model_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# State for the sub-graph\n",
    "class ExecAgentState(TypedDict):\n",
    "    # The input string\n",
    "    input: str\n",
    "    # The list of previous messages in the conversation\n",
    "    chat_history: Sequence[BaseMessage]\n",
    "    # The outcome of a given call to the agent\n",
    "    # Needs `None` as a valid type, since this is what this will start as\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    # List of actions and corresponding observations\n",
    "    # Here we annotate this with `operator.add` to indicate that operations to\n",
    "    # this state should be ADDED to the existing values (not overwrite it)\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "\n",
    "def should_continue(data):\n",
    "    # If the agent outcome is an AgentFinish, then we return `exit` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    if isinstance(data[\"agent_outcome\"], AgentFinish):\n",
    "        return \"end\"\n",
    "    # Otherwise, an AgentAction is returned\n",
    "    # Here we return `continue` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "async def run_agent(data):\n",
    "    agent_outcome = await agent_runnable.ainvoke(data)\n",
    "    return {\"agent_outcome\": agent_outcome}\n",
    "\n",
    "async def execute_tools(data):\n",
    "    # Get the most recent agent_outcome - this is the key added in the `agent` above\n",
    "    agent_action = data[\"agent_outcome\"]\n",
    "    if not isinstance(agent_action, list):\n",
    "        agent_action = [agent_action]\n",
    "    output = await tool_executor.abatch(agent_action, return_exceptions=True)\n",
    "    return {\n",
    "        \"intermediate_steps\": [\n",
    "            (action, str(out)) for action, out in zip(agent_action, output)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Define a new graph\n",
    "subworkflow = StateGraph(ExecAgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "subworkflow.add_node(\"subagent\", run_agent)\n",
    "subworkflow.add_node(\"action\", execute_tools)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "subworkflow.set_entry_point(\"subagent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "subworkflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"subagent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "subworkflow.add_edge(\"action\", \"subagent\")\n",
    "\n",
    "executor_agent = subworkflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the capital of France?',\n",
       " 'chat_history': [],\n",
       " 'agent_outcome': AgentFinish(return_values={'output': 'The capital of France is Paris.'}, log='The capital of France is Paris.'),\n",
       " 'intermediate_steps': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check the subgraph can be invoked in isolation\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "inputs = {\"input\": \"What is the capital of France?\", \"chat_history\": []}\n",
    "await executor_agent.ainvoke(inputs, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the main nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Create a plan based on the input, note how input from state is passed to the planner\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"objective\": state[\"input\"], \"chat_history\": state[\"messages\"]})\n",
    "    # The pydantic models will convert the output of the planner into a Plan object\n",
    "    return {\"plan\": plan.steps, \"response\": \"\"}\n",
    "\n",
    "# Execute a step using the shared state\n",
    "async def execute_step(state: PlanExecute):\n",
    "    task = state[\"plan\"][0] # the next step to execute\n",
    "    agent_response = await executor_agent.ainvoke({\"input\": task, \"chat_history\": state[\"messages\"]})\n",
    "    # Return the state with the current step and the new output added to past steps.\n",
    "    # Note: chat_history will be updated at the final response point.\n",
    "    return {\"past_steps\": (task, agent_response['agent_outcome'].return_values['output'])}\n",
    "\n",
    "# Replan based on the current state\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke(state)\n",
    "    if isinstance(output, Response):\n",
    "        # Chat history will now be updated with the final response as a list of AIMessages\n",
    "        return {\"response\": output.response, \"messages\": [AIMessage(content=output.response)]}\n",
    "    else:\n",
    "        return {\"plan\": output.steps}\n",
    "\n",
    "# decide if we should continue or return to the user\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the main graph\n",
    "\n",
    "We can now put it all together and define the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Create a new workflow\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# add planning node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# add execution node\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# add replanning node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "# Set entry\n",
    "workflow.set_entry_point(\"planner\")\n",
    "\n",
    "# Create edges\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    {\n",
    "        True: END,\n",
    "        False: \"agent\",\n",
    "    }\n",
    ")\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it!\n",
    "\n",
    "We can now use it!\n",
    "This now exposes the [same interface](https://python.langchain.com/docs/expression_language/) as all other LangChain runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': ['Identify the winner of the 2020 US presidential election.', \"Research the identified winner's place of birth or commonly known hometown.\"], 'response': ''}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'past_steps': ('Identify the winner of the 2020 US presidential election.', 'The winner of the 2020 US presidential election was Joe Biden.')}\n",
      "{'plan': [\"Research Joe Biden's place of birth or commonly known hometown.\"]}\n",
      "{'past_steps': (\"Research Joe Biden's place of birth or commonly known hometown.\", 'Joe Biden was born in Scranton, Pennsylvania, and his hometown is Wilmington, Delaware.')}\n",
      "{'messages': [AIMessage(content='Joe Biden, the winner of the 2020 US presidential election, was born in Scranton, Pennsylvania, and his hometown is Wilmington, Delaware.')], 'response': 'Joe Biden, the winner of the 2020 US presidential election, was born in Scranton, Pennsylvania, and his hometown is Wilmington, Delaware.'}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"4\",\"recursion_limit\": 50}}\n",
    "inputs = {\n",
    "    \"input\": \"What is the hometown of the 2020 US presidential election winner?\"\n",
    "}\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)\n",
    "            if \"response\" in v:\n",
    "                print(f\"End of conversation, final output: {v['response']}\")\n",
    "        else:\n",
    "            print(f\"End of conversation, final output: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue the conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': [\"Identify Joe Biden's Vice President during his term starting in 2021.\", 'Research the hometown of the identified Vice President.', 'Report the hometown of the Vice President.'], 'response': ''}\n",
      "{'past_steps': (\"Identify Joe Biden's Vice President during his term starting in 2021.\", \"Joe Biden's Vice President during his term starting in 2021 is Kamala Harris.\")}\n",
      "{'plan': ['Research the hometown of Kamala Harris.', 'Report the hometown of Kamala Harris.']}\n",
      "{'past_steps': ('Research the hometown of Kamala Harris.', \"Kamala Harris's hometown is Oakland, California.\")}\n",
      "{'messages': [AIMessage(content=\"Kamala Harris's hometown is Oakland, California.\")], 'response': \"Kamala Harris's hometown is Oakland, California.\"}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"4\",\"recursion_limit\": 50}}\n",
    "inputs = {\n",
    "    \"input\": \"What was the hometown of his VP?\"\n",
    "}\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding thoughts\n",
    "\n",
    "So if you want a StateGraph that include persistence, you need to consider how you define the input or messages component to allow for additional input. Considering the plan-and-execute model, the example implementations generally have the chat history as a placeholder prior to the input. In this case new input would replace the existing input in the state prompt pulled back into the model, so this may work. But this needs to be considered more before I decide for sure that works that way.\n",
    "\n",
    "Because the actual state object has the keys `input`, `plan`, `past_steps`, and `response`. There is no place to access new human input unless they were appended to `past_steps`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
